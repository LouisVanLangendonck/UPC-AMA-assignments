---
title: "08_IML_assignment"
author: "Louis Van Langendonck & Antonin Rosa & Albert Mart√≠n"
date: "`r format(Sys.time(), '%d/%b/%Y')`"
output:
  pdf_document: default
  html_document: default
---

## 0. Data Processing

Loading in data and library. Creating a training sample choosing 700 data at random. The non-chosen data will be the test set.

```{r}
library('readxl')
library('ranger')
library('vip')
library('gridExtra')
library('fastshap')
library('mgcv')
library('grid')
library('ggplot2')
library('DALEX')
library('lime')
library('DALEXtra')
```


```{r}
set.seed(123)

concrete <- as.data.frame(read_excel("Concrete_Data.xls"))
DescVars <- names(concrete)
names(concrete) <- c("Cement","Slag","FlyAsh","Water","Superplast",
"CoarseAggr","FineAggr","Age","Strength")

train.rows <- sample(nrow(concrete), 700)

data.train <- concrete[train.rows,]
data.test <- concrete[-train.rows,]

head(data.test)
```

## 1. Fit a Random Forest

```{r}
rf.model.impurity <- ranger(formula = Strength ~ .,data = data.train, importance='impurity')
rf.model.permutation <- ranger(formula = Strength ~ .,data = data.train, importance='permutation')

pred.fun <- function(object, newdata) {
  setNames(predict(object, newdata)$predictions, row.names(newdata)) # Exact same format as predict.lm to avoid errors
}

set.seed(123)

rf.imp.vip <- vip(rf.model.impurity, num_features = 8)
rf.perm.vip <- vip(rf.model.permutation, num_features = 8)
rf.shapley <- vip(rf.model.impurity, method="shap",
                  pred_wrapper=pred.fun, num_features = 8,
                  train = data.train, newdata=data.test[,1:8])

grid.arrange(rf.imp.vip, rf.perm.vip, rf.shapley,
             ncol=2, nrow=2,
             top="Top left: Impurity. Top right: oob permutations. Bottom left: Shapley values"
            )
```


## 2. Fit a linear model and a gam model

```{r}
lm.model <- lm(Strength ~ ., data = data.train)
summary(lm.model)
```

```{r}
gam1 <- gam(Strength ~ s(Cement) + s(Slag) + s(FlyAsh) + s(Water) + s(Superplast) + s(CoarseAggr) + s(FineAggr) + s(Age), data = data.train)
summary(gam1)

gam2 <- gam(Strength ~ s(Cement) + s(Slag) + s(FlyAsh) + s(Water) + s(Superplast) + CoarseAggr + s(FineAggr) + s(Age), data = data.train)
summary(gam2)

plot(gam1)
```

```{r}
lm.shapley <- vip(lm.model, method="shap",
                  pred_wrapper=predict.lm,
                  train=data.train, # train set must be specified
                  newdata=data.test, 
                  num_features = 8,
                  exact=TRUE)

gam.shapley1 <- vip(gam1, method="shap",
                  pred_wrapper=predict.gam,
                  train=data.train, # train set must be specified
                  newdata=data.test[,1:8], 
                  num_features = 8,
                  exact=TRUE) 

gam.shapley2 <- vip(gam2, method="shap",
                  pred_wrapper=predict.gam,
                  train=data.train, # train set must be specified
                  newdata=data.test[,1:8], 
                  num_features = 8,
                  exact=TRUE) 

grid.arrange(rf.shapley, lm.shapley, gam.shapley1, 
             gam.shapley2, 
             ncol=2, nrow=2,
             top="1,1: Shapley. 1,2: Shapley lm. 2,1: Shapley gam1. 2,2: Shapley gam2"
)
```

1. **Consistency in Top Three Parameters:**
   - The fact that the top three important parameters are consistent across all three models suggests that these variables have a strong influence on model predictions regardless of the modeling approach. This consistency provides confidence in the importance of these features.

2. **Differences in Second Most Important Parameter:**
   - The difference in the second most important parameter indicates that the models attribute varying levels of significance to this particular feature. This could be due to the nature of the data and the specific relationships captured by each model.

3. **Variability in Other Parameters:**
   - The variations in importance of other parameters highlight the divergent feature importance assessments made by different models. This could be because of the complexity captured by GAM models, which allow for more flexible relationships between variables compared to the linear model.

4. **Importance of "CoarseAggr" and "Water":**
   - The shift in importance for "CoarseAggr" between `gam2` and the linear model may indicate that the relationship of this variable with the response is better captured or more emphasized in `gam2`. Similarly, the absence of "water" among the top three important parameters in any of the models suggests that this variable might not have a strong impact on the predictions in these particular models.

5. **Lower Maximum Importance Values in Initial Values:**
   - The lower maximum importance values in the initial values compared to the three models might be due to the Shapley values being normalized or scaled. This scaling is often applied to ensure that the sum of Shapley values across all features equals the model's output for a given instance.


## 3. Relevance by Ghost Variables

Note that the Relevant Ghost Variable function provided by the professor does not work on the ranger random forest model. To solve it, two adaptations are made to the function:

1) Now, a user-specified prediction function has to be given (analogous to the vip function) to fix error based on predict.ranger outputs being of a different format than 'predict.lm' and 'predict.gam' etc.

2) in checking the class model for defining the 'term.labels' variable, an extra check for 'class(model)[1]!="ranger"' is added.

```{r}
source("relev.ghost.var.R")

rf.gv <- relev.ghost.var(model=rf.model.impurity, 
                              newdata = data.test[, -9],
                              y.ts = data.test[, 9],
                              func.model.ghost.var = lm,
                              pred.wrapper = pred.fun
)

plot.relev.ghost.var(rf.gv,n1=500,ncols.plot = 4)
```

```{r}
lm.gv <- relev.ghost.var(model=lm.model, 
                              newdata = data.test[, -9],
                              y.ts = data.test[, 9],
                           func.model.ghost.var = lm,
                           pred.wrapper = predict.lm
)
plot.relev.ghost.var(lm.gv,n1=500,ncols.plot = 4)
```

```{r}
gam.gv <- relev.ghost.var(model=gam1, 
                              newdata = data.test[, -9],
                              y.ts = data.test[, 9],
                           func.model.ghost.var = lm, 
                           pred.wrapper = predict.gam
)
plot.relev.ghost.var(gam.gv,n1=500,ncols.plot = 4)
```

## 4. Global Importance Measures and Plots using the library DALEX

```{r}
explainer_rf <- explain.default(model = rf.model.impurity,  
                               data = data.test[, -9],
                               y = data.test[, 9], 
                               label = "Random Forest")
```

```{r}
Rnd_Perm <- model_parts(
  explainer_rf,
  N = NULL, # All available data are used
  B = 10   # number of permutations to be used, with B = 10 used by default
)

Rnd_Perm
plot(Rnd_Perm)
```

```{r}
aux.plot <- plot(Rnd_Perm)
dropout_loss.y <- Rnd_Perm$dropout_loss[1]
aux.I <- order(-aux.plot$data$dropout_loss.x)
rf_perm_DALEX_as_vi <- tibble::tibble(aux.plot$data[aux.I,c(2,4)])
class(rf_perm_DALEX_as_vi) <- c("vi", class(rf_perm_DALEX_as_vi))
names(rf_perm_DALEX_as_vi) <- c("Variable", "Importance")
rf_perm_DALEX_as_vi$Importance <- 
  rf_perm_DALEX_as_vi$Importance - dropout_loss.y

# Creating the ggpolt: 
rf.perm.DALEX.vip <- vip(rf_perm_DALEX_as_vi)

grid.arrange(rf.imp.vip, rf.perm.vip,
             rf.perm.DALEX.vip, ncol=2, nrow=2,
             top="Top left: Impurity. Top right: oob permutations. Bottom left: test sample permutations"
             )
```

```{r}
# ?model_profile
PDP_rf <- model_profile(
  explainer=explainer_rf,
  variables = NULL,  # All variables are used
  N = NULL, # All available data are used
  groups = NULL,
  k = NULL,
  center = TRUE,
  type = "partial" #  partial, conditional or accumulated
)

plot(PDP_rf, facet_ncol=2)
```

```{r}
CDP_rf <- model_profile(
  explainer=explainer_rf,
  variables = NULL,  # All variables are used
  N = NULL, # All available data are used
  groups = NULL,
  k = NULL,
  center = TRUE,
  type = "conditional" #  partial, conditional or accumulated
)

plot(CDP_rf, facet_ncol=2)
```

```{r}
ALE_rf <- model_profile(
  explainer=explainer_rf,
  variables = NULL,  # All variables are used
  N = NULL, # All available data are used
  groups = NULL,
  k = NULL,
  center = TRUE,
  type = "accumulated" #  partial, conditional or accumulated
)

plot(ALE_rf, facet_ncol=2)
```

## 5. Local Explainers with the library DALEX

```{r}
point.min.strength <- data.test[which.min(data.test[,9]),-9]
point.max.strength <- data.test[which.max(data.test[,9]),-9]

shap.rf.min <- predict_parts(explainer = explainer_rf,
                 new_observation = point.min.strength,
                            type = "shap")

plot(shap.rf.min)

shap.rf.max <- predict_parts(explainer = explainer_rf,
                 new_observation = point.max.strength,
                            type = "shap")

plot(shap.rf.max)
```

**SHAP Analysis Summary:**

The SHAP analysis reveals both consistencies and divergences in feature importance between max and min strength. The similar order of importance magnitudes indicates stability in the overall influence of features. However, the inversion and discrepancy in contribution magnitudes highlight nuanced variations in how specific features impact predictions based on the strength context. The divergent directions of contributions underscore the model's differential response to features when predicting max and min strength. Understanding these patterns can provide deeper insights into how the model perceives and weighs different factors under different strength conditions.


```{r}
bd.rf.min <- predict_parts(explainer = explainer_rf,
                 new_observation = point.min.strength,
                            type = "break_down")

plot(bd.rf.min)

bd.rf.max <- predict_parts(explainer = explainer_rf,
                 new_observation = point.max.strength,
                            type = "break_down")

plot(bd.rf.max)
```

**Summary:**
The observed variations in the breakdown profile between max strength and min strength underscore the model's sensitivity to different conditions, emphasizing the need for a careful analysis of each feature's contribution based on the prediction context. The substantial difference in the overall prediction value reinforces the notion that the model responds differently to scenarios of maximum and minimum strength. A thorough analysis of these differences can yield valuable insights into understanding the model and its behavior in specific situations.


Because the 'DALEXtra' package was built under another version of R, the following function for lime-like analysis does not work anymore. 

```{r}
#lime_rf <- predict_surrogate(explainer = explainer_rf,  DOES NOT WORK
                  #new_observation = data.test[,-9], 
                  #type = "localModel") 
                  #type = "iml") # it does not work
                  #n_features = 6, 
                  #n_permutations = 1000,
                  #type = "lime") # it does not work
```

Therefore we have the following solution. This is a special package that does LIME analysis. 

```{r}
explainer.rf.lime <- lime(data.test, rf.model.impurity)
lime.explain.min <- lime::explain(point.min.strength, explainer.rf.lime, n_features = 5)
plot_features(lime.explain.min)
lime.explain.max <- lime::explain(point.max.strength, explainer.rf.lime, n_features = 5)
plot_features(lime.explain.max)
```

The consistency in the order of contributions in absolute values suggests stability in how features contribute to the model predictions. This can enhance confidence in the role of these features in the model, regardless of variations in the target variable "Strength."


```{r}
ice.rf.min <- predict_profile(explainer = explainer_rf, 
                           new_observation = point.min.strength)
plot(ice.rf.min, facet_ncol=3)

ice.rf.max <- predict_profile(explainer = explainer_rf, 
                           new_observation = point.max.strength)
plot(ice.rf.max, facet_ncol=3)
```

```{r}
mp_rf <- model_profile(explainer = explainer_rf,
  variables = "Age",
  N = NULL, #All observations in test_set
  type = "partial"
)

?model_profile()

plot(mp_rf, geom = "profiles") +  
  ggtitle("Ceteris-paribus and partial-dependence profiles for Age") 
```
