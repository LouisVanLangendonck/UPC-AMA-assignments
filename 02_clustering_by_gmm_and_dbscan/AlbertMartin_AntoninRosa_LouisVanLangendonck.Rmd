---
title: "Density estimation. GMM. DBSCAN (Assignment)"
author: "Louis Van Langendonck & Antonin Rosa & Albert Mart√≠n"
date: "`r format(Sys.time(), '%d/%b/%Y')`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
load("BikeDay.Rdata")
X <- as.matrix(day[day$yr==1,c(10,14)])
#pairs(X)
```

## 1. model based clustering assuming a Gaussian Mixture Model

Using the G parameter in the `Mclust` function, options for K are set between 2 and 6. It automatically selects the model with the highest BIC score. Via the `modelNames` parameter, mixture components are allowed variation in volume, shape, and orientation. The 'BIC'-plot below shows that K = 3 scores the best and is thus the choice of K the model automatically continues with. The classification, uncertainty and density plots use this value of K

```{r ,warning=FALSE,message=FALSE,out.height='40%'}
library(mclust)
GMM <- Mclust(X,G=2:6,modelNames="VVV")
plot(GMM, "BIC")
plot(GMM, "classification")
plot(GMM, "uncertainty")
plot(GMM, "density")
```

## 2. Compare the previous density plot with the non-parametric density estimation of (temp,casual).

Comparing these plots, it becomes clear both show a similar profile. However the GMM model (left) yields a clear distinction in three gaussian shapes (as typically predicted in this type of model), while the non-parametric model (right) shows more arbitrary outlines. This is due to the fact that in the first case, the data is fit to a mix of parametric model (mixture of three gaussians) and the plot is a clear visual representation of those three gaussians, while the second plot is a direct representation of the data density, and thus less regular.

```{r ,warning=FALSE,message=FALSE}
library(sm)
par(mfrow = c(1, 2))
plot(GMM,"density",main="GMM")
a <- 0.25
h_prop <- a*c(sd(X[,"temp"]), sd(X[,"casual"]))
sm.density(X,h_prop,display="slice",main="Non-param Denisty Estimator")
```

## 3. Non-parametric joint density estimation of each of the clusters

Below the estimated bivariate densities are plotted for each of the three clusters.

```{r}
clust.ind <- GMM$classification
plot(X,col=clust.ind)
for (j in 1:3){
  cl.j <- (clust.ind==j)
  sm.density(
    X[cl.j,],
    h=0.4*c(sd(X[cl.j,"temp"]),sd(X[cl.j,"casual"])),
    display="slice",
    props=c(75),
    col=j, 
    cex=4, 
    add=TRUE)
}
```

## 4. Component merging

the ```fpc::mergenormals``` functions proposes a merge from three to two clusters. The result can be found below.

```{r,warning=FALSE,message=FALSE}
library(fpc)
GMMbic <- mclustBIC(X,G=3,modelnames='VVV')
GMMmerge <- fpc::mergenormals(X,summary(GMMbic,X),method = "bhat")
GMMmerge.ind <- GMMmerge$clustering
plot(X,col=GMMmerge.ind)
```

## 5. Non-param density estimation of each of the new, merged clusters

Again, non-parametric density estimation is applied on each of the clusters (that are now just two).

```{r}
plot(X,col=GMMmerge.ind)
for (j in 1:2){
  cl.j <- (GMMmerge.ind==j)
  sm.density(X[cl.j,],
             h=0.4*c(sd(X[cl.j,"temp"]),sd(X[cl.j,"casual"])),
             display="slice",
             props=c(75),
             col=j, 
             cex=4, 
             add=TRUE)
}
```


## 6. DBScan

All combinations of parameters are tested, and the average silhouette across all clusters is computed for each of them. We have chosen the ideal parameters as those that lead to the highest average silhouette. 

```{r,warning=FALSE,message=FALSE}
library(dbscan)
library(cluster)

Xs <- scale(X)

eps_values <- c(0.25, 0.5)
minPts_values <- c(10, 15, 20)
results <- data.frame()

best_silhouette <- -Inf
best_eps <- NULL
best_minPts <- NULL

for (eps in eps_values) {
  for (minPts in minPts_values) {
    #Compute DBscan
    dbscan_result <- dbscan(Xs, eps = eps, minPts = minPts)
    
    #Compute Silhouette,as long as there is more than a single cluster
    if (max(dbscan_result$cluster) > 1) {
      silhouette_avg <- silhouette(dbscan_result$cluster, dist(Xs))
      avg_silhouette <- mean(silhouette_avg[, "sil_width"])
      
      results <- rbind(results, 
                       data.frame(eps = eps, minPts = minPts, silhouette = avg_silhouette))
    
      if (avg_silhouette > best_silhouette) {
        best_silhouette <- avg_silhouette
        best_eps <- eps
        best_minPts <- minPts
      }
    }
  }
}

cat("Best combination of parameters:\n")
cat("Epsilon:", best_eps, "\n")
cat("MinPts:", best_minPts, "\n")

dbscan_result <- dbscan(Xs, eps = best_eps, minPts = best_minPts)
plot(Xs, col = dbscan_result$cluster, pch = 19, main = "DBSCAN Clustering")

contingency_table <- table(dbscan = dbscan_result$cluster, GMM = GMMmerge.ind)

print(contingency_table)

```

Even though DBSCAN Cluster 0 (representing noise or unclustered data) is attributed to two distinct clusters by GMM (GMM Cluster 0 and GMM Cluster 1), the total number of points in this cluster is relatively low (7 points in total).

Overall, these results indicate a strong similarity between the clustering results of DBSCAN and GMM for most observations. Ambiguity is primarily found in DBSCAN Cluster 0, representing noise or unclustered data. This similarity may suggest that both methods are uncovering similar data structures, at least for the data that is meaningfully clustered.

## 7. Interpretation.
In order to analyze the meaning of the two clustered data points, we visualized the distributions of every variable conditioned on the assigned cluster. For categorical features we show the level frequencies conditioned on the cluster, and for the numerical ones we show the histograms, also for both clusters.

Looking at the numerical features, the first thing one can notice is that weather conditions for the second cluster tend to have less variance and generally be better, so we can confidently hypothesize that the factors determining the cluster will have something to do with good weather.

Of course, we see the expected difference in the casual feature as well, as it also defined the clustering process along with the temperature. The histograms show how cluster 1 tends to have lower casual values while cluster 2 has the highest, and also days assigned in cluster 2 have much higher total count of users. Noticing that the mean of registered users is very similar in days of both clusters while the number if casual users is significantly larger for cluster 2 days, along with the better temperatures on those days, we believe we can explain this behavior by assuming that in cluster 2 days there is increased demand of bicycles for leisure use. This would also explain why the number of registered users does not change much, as those use the service for their work days as well as for leisure days, while the other users do not have registration and only use it for leisure time, thus increasing the casual user count on cluster 2 days.

This hypothesis is further reinforced by looking at the categorical feature distributions depending on the cluster. Just by checking season and month, one can already assert that cluster 2 days are never during winter season, which makes sense considering Washington has a cold climate, making leisure strolls in a bicycle much less appealing. Finally, the weekday and working day features imply that days assigned to cluster 2 are weekends, which would fit with our hypothesis of cluster 2 days being days where users use the service for leisure.

Thus, after analyzing how the features are distributed depending on the cluster they are assigned to, we believe these clusters indicate two different kinds of days for the service. 
* Cluster 1: Work days where registered users use the service to commute to work.
* Cluster 2: Weekends or days with good temperature where both registered and casual users use the service for leisure.

```{r out.height='40%'}
df<-data.frame(day[day$yr==1,])
catCols<-c(3,5,6,7,8,9)
numCols<-c(1,10,11,12,13,14,15,16)
numdf<-df[,numCols]
catdf<-data.frame(lapply(df[,catCols],factor))
catdf$cluster<-factor(dbscan_result$cluster)
numdf1<-numdf[catdf$cluster==1,]
numdf2<-numdf[catdf$cluster==2,]
catdf1<-catdf[catdf$cluster==1,]
catdf2<-catdf[catdf$cluster==2,]
par(mfrow = c(1, 2))
#Season
plot(catdf1$season,main="Cluster 1: Season")
plot(catdf2$season,main="Cluster 2: Season")
#Month
plot(catdf1$mnth,main="Cluster 1: Month")
plot(catdf2$mnth,main="Cluster 2: Month")
#Holiday
plot(catdf1$holiday,main="Cluster 1: Holiday")
plot(catdf2$holiday,main="Cluster 2: Holiday")
#weekday
plot(catdf1$weekday,main="Cluster 1: Weekday")
plot(catdf2$weekday,main="Cluster 2: Weekday")
#workingday
plot(catdf1$workingday,main="Cluster 1: Workingday")
plot(catdf2$workingday,main="Cluster 2: Workingday")
#Weathersit
plot(catdf1$weathersit,main="Cluster 1: Weathersit")
plot(catdf2$weathersit,main="Cluster 2: Weathersit")

#Temperature
hist(numdf1$temp,main="Cluster 1: Temperature",xlim = c(0,1))
hist(numdf2$temp,main="Cluster 2: Temperature",xlim = c(0,1))
#Feeling temperature
hist(numdf1$atemp,main="Cluster 1: Feeling temperature",xlim = c(0,1))
hist(numdf2$atemp,main="Cluster 2: Feeling temperature",xlim = c(0,1))
#Humidity
hist(numdf1$hum,main="Cluster 1: Humidity",xlim = c(0,1))
hist(numdf2$hum,main="Cluster 2: Humidity",xlim = c(0,1))
#Wind Speed
hist(numdf1$windspeed,main="Cluster 1: Windspeed",xlim = c(0,0.55))
hist(numdf2$windspeed,main="Cluster 2: Windspeed",xlim = c(0,0.55))
#Casual
hist(numdf1$casual,main="Cluster 1: Casual",xlim = c(0,3300))
hist(numdf2$casual,main="Cluster 2: Casual",xlim = c(0,3300))
#Registered
hist(numdf1$registered,main="Cluster 1: Registered",xlim = c(0,7000))
hist(numdf2$registered,main="Cluster 2: Registered",xlim = c(0,7000))
#Count
hist(numdf1$cnt,main="Cluster 1: Count",xlim = c(0,10000))
hist(numdf2$cnt,main="Cluster 2: Count",xlim = c(0,10000))


```
